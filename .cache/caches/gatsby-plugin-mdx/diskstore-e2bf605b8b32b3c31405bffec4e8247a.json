{"expireTime":9007200852781510000,"key":"gatsby-plugin-mdx-entire-payload-782acc281cd4076dcec25775d8cda4a7-","val":{"mdast":{"type":"root","children":[{"type":"heading","depth":3,"children":[{"type":"text","value":"Streams and Buffer","position":{"start":{"line":1,"column":5,"offset":4},"end":{"line":1,"column":23,"offset":22},"indent":[]}}],"position":{"start":{"line":1,"column":1,"offset":0},"end":{"line":1,"column":23,"offset":22},"indent":[]}},{"type":"paragraph","children":[{"type":"text","value":"Streams are used to get very large data in a progressive way so that the\nserver doesn't need to wait till the whole data is brought. So Reading as Chunks of Data. Basically it's just like streaming a video.","position":{"start":{"line":2,"column":1,"offset":23},"end":{"line":3,"column":134,"offset":229},"indent":[1]}}],"position":{"start":{"line":2,"column":1,"offset":23},"end":{"line":3,"column":134,"offset":229},"indent":[1]}},{"type":"paragraph","children":[{"type":"text","value":"Read & Write Stream","position":{"start":{"line":5,"column":1,"offset":231},"end":{"line":5,"column":20,"offset":250},"indent":[]}}],"position":{"start":{"line":5,"column":1,"offset":231},"end":{"line":5,"column":20,"offset":250},"indent":[]}},{"type":"code","lang":"javascript","meta":null,"value":"const readStream = fs.createReadStream(\"./docs/long-data.txt\", {\n encoding: \"utf8\",\n});\n//Make Enoding to Utf-8 in readStream to avoid toString Method\n\nconst writeStream = fs.createWriteStream(\"./docs/write-stream.txt\");","position":{"start":{"line":6,"column":1,"offset":251},"end":{"line":13,"column":4,"offset":489},"indent":[1,1,1,1,1,1,1]}},{"type":"paragraph","children":[{"type":"text","value":"Reading Chunk of Data and Writing to another file","position":{"start":{"line":15,"column":1,"offset":491},"end":{"line":15,"column":50,"offset":540},"indent":[]}}],"position":{"start":{"line":15,"column":1,"offset":491},"end":{"line":15,"column":50,"offset":540},"indent":[]}},{"type":"code","lang":"javascript","meta":null,"value":"readStream.on(\"data\", (chunk) => {\nconsole.log(\"---------Chunk---------\");\nconsole.log(chunk);\n\nwriteStream.write(\"--------Chunk--------\");\nwriteStream.write(chunk);\n});","position":{"start":{"line":17,"column":1,"offset":542},"end":{"line":25,"column":4,"offset":729},"indent":[1,1,1,1,1,1,1,1]}},{"type":"paragraph","children":[{"type":"text","value":"This Whole Process of Reading Chunks and Writing the Same to another file can be implemented using pipes, which simplifies the above code","position":{"start":{"line":27,"column":1,"offset":731},"end":{"line":27,"column":138,"offset":868},"indent":[]}}],"position":{"start":{"line":27,"column":1,"offset":731},"end":{"line":27,"column":138,"offset":868},"indent":[]}},{"type":"code","lang":"javascript","meta":null,"value":"readStream = pipe(writeStream);","position":{"start":{"line":29,"column":1,"offset":870},"end":{"line":31,"column":4,"offset":919},"indent":[1,1]}},{"type":"export","value":"export const _frontmatter = {\"title\":\"Streams and Buffer\"}","position":{"start":{"line":33,"column":1,"offset":921},"end":{"line":33,"column":59,"offset":979},"indent":[]}}],"position":{"start":{"line":1,"column":1,"offset":0},"end":{"line":33,"column":59,"offset":979}}},"scopeImports":["import * as React from 'react'"],"scopeIdentifiers":["React"],"body":"function _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"Streams and Buffer\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h3\", null, \"Streams and Buffer\"), mdx(\"p\", null, \"Streams are used to get very large data in a progressive way so that the\\nserver doesn't need to wait till the whole data is brought. So Reading as Chunks of Data. Basically it's just like streaming a video.\"), mdx(\"p\", null, \"Read & Write Stream\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-javascript\"\n  }), \"const readStream = fs.createReadStream(\\\"./docs/long-data.txt\\\", {\\n encoding: \\\"utf8\\\",\\n});\\n//Make Enoding to Utf-8 in readStream to avoid toString Method\\n\\nconst writeStream = fs.createWriteStream(\\\"./docs/write-stream.txt\\\");\\n\")), mdx(\"p\", null, \"Reading Chunk of Data and Writing to another file\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-javascript\"\n  }), \"readStream.on(\\\"data\\\", (chunk) => {\\nconsole.log(\\\"---------Chunk---------\\\");\\nconsole.log(chunk);\\n\\nwriteStream.write(\\\"--------Chunk--------\\\");\\nwriteStream.write(chunk);\\n});\\n\")), mdx(\"p\", null, \"This Whole Process of Reading Chunks and Writing the Same to another file can be implemented using pipes, which simplifies the above code\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-javascript\"\n  }), \"readStream = pipe(writeStream);\\n\")));\n}\n;\nMDXContent.isMDXComponent = true;","rawMDXOutput":"/* @jsx mdx */\nimport { mdx } from '@mdx-js/react';\n/* @jsx mdx */\n\nexport const _frontmatter = {\n  \"title\": \"Streams and Buffer\"\n};\n\nconst layoutProps = {\n  _frontmatter\n};\nconst MDXLayout = \"wrapper\"\nexport default function MDXContent({\n  components,\n  ...props\n}) {\n  return <MDXLayout {...layoutProps} {...props} components={components} mdxType=\"MDXLayout\">\n    <h3>{`Streams and Buffer`}</h3>\n    <p>{`Streams are used to get very large data in a progressive way so that the\nserver doesn't need to wait till the whole data is brought. So Reading as Chunks of Data. Basically it's just like streaming a video.`}</p>\n    <p>{`Read & Write Stream`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-javascript\"\n      }}>{`const readStream = fs.createReadStream(\"./docs/long-data.txt\", {\n encoding: \"utf8\",\n});\n//Make Enoding to Utf-8 in readStream to avoid toString Method\n\nconst writeStream = fs.createWriteStream(\"./docs/write-stream.txt\");\n`}</code></pre>\n    <p>{`Reading Chunk of Data and Writing to another file`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-javascript\"\n      }}>{`readStream.on(\"data\", (chunk) => {\nconsole.log(\"---------Chunk---------\");\nconsole.log(chunk);\n\nwriteStream.write(\"--------Chunk--------\");\nwriteStream.write(chunk);\n});\n`}</code></pre>\n    <p>{`This Whole Process of Reading Chunks and Writing the Same to another file can be implemented using pipes, which simplifies the above code`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-javascript\"\n      }}>{`readStream = pipe(writeStream);\n`}</code></pre>\n\n    </MDXLayout>;\n}\n\n;\nMDXContent.isMDXComponent = true;"}}