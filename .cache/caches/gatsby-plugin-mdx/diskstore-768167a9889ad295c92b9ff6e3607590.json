{"expireTime":9007200852810991000,"key":"gatsby-plugin-mdx-entire-payload-2acaba8c63c89a0a2f6e33603369ccbf-","val":{"mdast":{"type":"root","children":[{"type":"heading","depth":2,"children":[{"type":"text","value":"Streams and Buffer","position":{"start":{"line":2,"column":4,"offset":4},"end":{"line":2,"column":22,"offset":22},"indent":[]}}],"position":{"start":{"line":2,"column":1,"offset":1},"end":{"line":2,"column":22,"offset":22},"indent":[]}},{"type":"paragraph","children":[{"type":"text","value":"Streams are used to get very large data in a progressive way so that the\nserver doesn't need to wait till the whole data is brought. So Reading as Chunks of Data. Basically it's just like streaming a video.","position":{"start":{"line":4,"column":1,"offset":24},"end":{"line":5,"column":134,"offset":230},"indent":[1]}}],"position":{"start":{"line":4,"column":1,"offset":24},"end":{"line":5,"column":134,"offset":230},"indent":[1]}},{"type":"paragraph","children":[{"type":"text","value":"Read & Write Stream","position":{"start":{"line":7,"column":1,"offset":232},"end":{"line":7,"column":20,"offset":251},"indent":[]}}],"position":{"start":{"line":7,"column":1,"offset":232},"end":{"line":7,"column":20,"offset":251},"indent":[]}},{"type":"code","lang":"javascript","meta":null,"value":"const readStream = fs.createReadStream('./docs/long-data.txt', {\n  encoding: 'utf8',\n});\n//Make Enoding to Utf-8 in readStream to avoid toString Method\n\nconst writeStream = fs.createWriteStream('./docs/write-stream.txt');","position":{"start":{"line":9,"column":1,"offset":253},"end":{"line":16,"column":4,"offset":492},"indent":[1,1,1,1,1,1,1]}},{"type":"paragraph","children":[{"type":"text","value":"Reading Chunk of Data and Writing to another file","position":{"start":{"line":18,"column":1,"offset":494},"end":{"line":18,"column":50,"offset":543},"indent":[]}}],"position":{"start":{"line":18,"column":1,"offset":494},"end":{"line":18,"column":50,"offset":543},"indent":[]}},{"type":"code","lang":"javascript","meta":null,"value":"readStream.on('data', chunk => {\n  console.log('---------Chunk---------');\n  console.log(chunk);\n\n  writeStream.write('--------Chunk--------');\n  writeStream.write(chunk);\n});","position":{"start":{"line":20,"column":1,"offset":545},"end":{"line":28,"column":4,"offset":738},"indent":[1,1,1,1,1,1,1,1]}},{"type":"paragraph","children":[{"type":"text","value":"This Whole Process of Reading Chunks and Writing the Same to another file can be implemented using pipes, which simplifies the above code","position":{"start":{"line":30,"column":1,"offset":740},"end":{"line":30,"column":138,"offset":877},"indent":[]}}],"position":{"start":{"line":30,"column":1,"offset":740},"end":{"line":30,"column":138,"offset":877},"indent":[]}},{"type":"code","lang":"javascript","meta":null,"value":"readStream = pipe(writeStream);","position":{"start":{"line":32,"column":1,"offset":879},"end":{"line":34,"column":4,"offset":928},"indent":[1,1]}},{"type":"export","value":"export const _frontmatter = {\"title\":\"Streams and Buffer\"}","position":{"start":{"line":37,"column":1,"offset":931},"end":{"line":37,"column":59,"offset":989},"indent":[]}}],"position":{"start":{"line":1,"column":1,"offset":0},"end":{"line":37,"column":59,"offset":989}}},"scopeImports":["import * as React from 'react'"],"scopeIdentifiers":["React"],"body":"function _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"Streams and Buffer\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h2\", null, \"Streams and Buffer\"), mdx(\"p\", null, \"Streams are used to get very large data in a progressive way so that the\\nserver doesn't need to wait till the whole data is brought. So Reading as Chunks of Data. Basically it's just like streaming a video.\"), mdx(\"p\", null, \"Read & Write Stream\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-javascript\"\n  }), \"const readStream = fs.createReadStream('./docs/long-data.txt', {\\n  encoding: 'utf8',\\n});\\n//Make Enoding to Utf-8 in readStream to avoid toString Method\\n\\nconst writeStream = fs.createWriteStream('./docs/write-stream.txt');\\n\")), mdx(\"p\", null, \"Reading Chunk of Data and Writing to another file\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-javascript\"\n  }), \"readStream.on('data', chunk => {\\n  console.log('---------Chunk---------');\\n  console.log(chunk);\\n\\n  writeStream.write('--------Chunk--------');\\n  writeStream.write(chunk);\\n});\\n\")), mdx(\"p\", null, \"This Whole Process of Reading Chunks and Writing the Same to another file can be implemented using pipes, which simplifies the above code\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-javascript\"\n  }), \"readStream = pipe(writeStream);\\n\")));\n}\n;\nMDXContent.isMDXComponent = true;","rawMDXOutput":"/* @jsx mdx */\nimport { mdx } from '@mdx-js/react';\n/* @jsx mdx */\n\nexport const _frontmatter = {\n  \"title\": \"Streams and Buffer\"\n};\n\nconst layoutProps = {\n  _frontmatter\n};\nconst MDXLayout = \"wrapper\"\nexport default function MDXContent({\n  components,\n  ...props\n}) {\n  return <MDXLayout {...layoutProps} {...props} components={components} mdxType=\"MDXLayout\">\n    <h2>{`Streams and Buffer`}</h2>\n    <p>{`Streams are used to get very large data in a progressive way so that the\nserver doesn't need to wait till the whole data is brought. So Reading as Chunks of Data. Basically it's just like streaming a video.`}</p>\n    <p>{`Read & Write Stream`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-javascript\"\n      }}>{`const readStream = fs.createReadStream('./docs/long-data.txt', {\n  encoding: 'utf8',\n});\n//Make Enoding to Utf-8 in readStream to avoid toString Method\n\nconst writeStream = fs.createWriteStream('./docs/write-stream.txt');\n`}</code></pre>\n    <p>{`Reading Chunk of Data and Writing to another file`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-javascript\"\n      }}>{`readStream.on('data', chunk => {\n  console.log('---------Chunk---------');\n  console.log(chunk);\n\n  writeStream.write('--------Chunk--------');\n  writeStream.write(chunk);\n});\n`}</code></pre>\n    <p>{`This Whole Process of Reading Chunks and Writing the Same to another file can be implemented using pipes, which simplifies the above code`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-javascript\"\n      }}>{`readStream = pipe(writeStream);\n`}</code></pre>\n\n    </MDXLayout>;\n}\n\n;\nMDXContent.isMDXComponent = true;"}}